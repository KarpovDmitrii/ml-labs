{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(r\"../data/winequality-red.csv\",decimal=\",\",delimiter=\";\")\n",
    "df2 = pd.read_csv(r\"../data/winequality-white.csv\",decimal=\",\",delimiter=\";\")\n",
    "df1.insert (loc= len(df1.columns) , column='color', value=6)\n",
    "df2.insert (loc= len(df2.columns) , column='color', value=7)\n",
    "data = pd.concat([df1, df2], axis = 0,ignore_index = True)\n",
    "data['fixed acidity'] = data['fixed acidity'].astype(float)\n",
    "data['volatile acidity'] = data['volatile acidity'].astype(float)\n",
    "data['citric acid'] = data['citric acid'].astype(float)\n",
    "data['residual sugar'] = data['residual sugar'].astype(float)\n",
    "data['chlorides'] = data['chlorides'].astype(float)\n",
    "data['free sulfur dioxide'] = data['free sulfur dioxide'].astype(float)\n",
    "data['total sulfur dioxide'] = data['total sulfur dioxide'].astype(float)\n",
    "data['density'] = data['density'].astype(float)\n",
    "data['pH'] = data['pH'].astype(float)\n",
    "data['sulphates'] = data['sulphates'].astype(float)\n",
    "data['alcohol'] = data['alcohol'].astype(float)\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.duplicated().sum()\n",
    "outlier=x = data.drop([\"color\", \"quality\"], axis=1)\n",
    "Q1 = outlier.quantile(0.25)\n",
    "Q3 = outlier.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "data_f = outlier[~((outlier < (Q1 - 1.5 * IQR)) | (outlier > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "index_list = list(data_f.index.values)\n",
    "data_f = data[data.index.isin(index_list)]\n",
    "datareg=data_f\n",
    "\n",
    "y_reg = data[\"quality\"]\n",
    "x_reg = data.drop(\"quality\", axis=1)\n",
    "x_train_r, x_test_r, y_train_r, y_test_r = train_test_split(x_reg, y_reg, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../data/weatherAUS.csv\")\n",
    "data.dropna(inplace=True,ignore_index=True)\n",
    "f = lambda x : str(x)[5:7]\n",
    "data['Date'] = data['Date'].transform(f)\n",
    "data['Date'] = data['Date'].astype(int)\n",
    "f = lambda x : 0 if (x == \"No\") else 1\n",
    "data['RainToday'] = data['RainToday'].transform(f)\n",
    "data['RainToday'] = data['RainToday'].astype(int)\n",
    "\n",
    "data['RainTomorrow'] = data['RainTomorrow'].transform(f)\n",
    "data['RainTomorrow'] = data['RainTomorrow'].astype(int)\n",
    "categorical_features = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm']\n",
    "categorical_encoder = OneHotEncoder(sparse_output=False)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "ct = ColumnTransformer(transformers=[\n",
    "     ('cat', categorical_encoder, categorical_features)\n",
    "     ])\n",
    "# Получите закодированные функции в виде DataFrame.\n",
    "ct.set_output(transform='pandas')\n",
    "encoded_features = ct.fit_transform(data)\n",
    "encoded_features.head()\n",
    "data=data.drop(['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm'],axis=1)\n",
    "df = pd.concat([\n",
    "    data,\n",
    "    encoded_features\n",
    "], axis=1)\n",
    "df_major_0 = df[df[\"RainTomorrow\"] == 0]\n",
    "df_minor_1 = df[df[\"RainTomorrow\"] == 1]\n",
    "\n",
    "df_major_0_undersampled = df_major_0.sample(len(df_minor_1))\n",
    "\n",
    "dataclass = pd.concat([df_major_0_undersampled, df_minor_1], axis=0)\n",
    "\n",
    "x_class=dataclass.drop(['RainTomorrow'],axis=1)\n",
    "y_class=dataclass['RainTomorrow']\n",
    "\n",
    "x_train_c, x_test_c, y_train_c, y_test_c = train_test_split(x_class, y_class, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      " dense_98 (Dense)            (None, 64)                832       \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_100 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3457 (13.50 KB)\n",
      "Trainable params: 3457 (13.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# создаем модель, как набор последовательных слоев\n",
    "model_regression = tf.keras.Sequential(\n",
    "    [\n",
    "        # Dense - полносвязный слой (каждый нейрон следующего слоя связан со всеми нейронами предыдущего)\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(12,)),\n",
    "        # на втором скрытом слое будет 32 нейрона\n",
    "        tf.keras.layers.Dense(32, activation=\"linear\"),\n",
    "        # Dropout позволяет внести фактор случайности - при обучении часть нейронов будет отключаться\n",
    "        # каждый нейрон, в данном случае, будет отключаться с вероятностью 0.1\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        # на выходе один нейрон, функция активации не применяется\n",
    "        tf.keras.layers.Dense(1, activation=\"linear\"),\n",
    "    ]\n",
    ")\n",
    "model_regression.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# компилируем\n",
    "model_regression.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 1s 1ms/step - loss: 8.3167\n",
      "Epoch 2/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 2.2387\n",
      "Epoch 3/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 1.9172\n",
      "Epoch 4/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 1.7997\n",
      "Epoch 5/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 1.6955\n",
      "Epoch 6/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 1.5082\n",
      "Epoch 7/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 1.4175\n",
      "Epoch 8/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 1.3786\n",
      "Epoch 9/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 1.2680\n",
      "Epoch 10/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 1.1897\n",
      "Epoch 11/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 1.1983\n",
      "Epoch 12/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 1.1024\n",
      "Epoch 13/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 1.0659\n",
      "Epoch 14/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 1.0048\n",
      "Epoch 15/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.9906\n",
      "Epoch 16/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.9487\n",
      "Epoch 17/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.8811\n",
      "Epoch 18/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.8637\n",
      "Epoch 19/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.8028\n",
      "Epoch 20/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.8046\n",
      "Epoch 21/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.7687\n",
      "Epoch 22/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.7659\n",
      "Epoch 23/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.7277\n",
      "Epoch 24/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.7164\n",
      "Epoch 25/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.6707\n",
      "Epoch 26/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.6818\n",
      "Epoch 27/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.6580\n",
      "Epoch 28/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.6707\n",
      "Epoch 29/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.6235\n",
      "Epoch 30/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.6247\n",
      "Epoch 31/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.6181\n",
      "Epoch 32/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.6121\n",
      "Epoch 33/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.5985\n",
      "Epoch 34/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.6102\n",
      "Epoch 35/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.5940\n",
      "Epoch 36/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.5816\n",
      "Epoch 37/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.5735\n",
      "Epoch 38/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.5794\n",
      "Epoch 39/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.5623\n",
      "Epoch 40/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.6010\n",
      "Epoch 41/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.5824\n",
      "Epoch 42/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.5705\n",
      "Epoch 43/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.5812\n",
      "Epoch 44/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.5836\n",
      "Epoch 45/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.5719\n",
      "Epoch 46/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.5943\n",
      "Epoch 47/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.5825\n",
      "Epoch 48/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.5755\n",
      "Epoch 49/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.5831\n",
      "Epoch 50/50\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 0.5804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2af411f91e0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_regression.fit(x_train_r, y_train_r, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 849us/step\n",
      "0.5784535645542288\n",
      "34/34 [==============================] - 0s 929us/step\n",
      "0.5537894641894647\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(y_test_r, model_regression.predict(x_test_r)))\n",
    "print(mean_squared_error(y_test_r, model_regression.predict(x_test_r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80      4099\n",
      "           1       0.80      0.79      0.80      4103\n",
      "\n",
      "    accuracy                           0.80      8202\n",
      "   macro avg       0.80      0.80      0.80      8202\n",
      "weighted avg       0.80      0.80      0.80      8202\n",
      "\n",
      "[[3298  801]\n",
      " [ 849 3254]]\n"
     ]
    }
   ],
   "source": [
    "model_classification_1 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(92,)),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        # сначала используем 1 нейрон и sigmoid\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "# в качестве функции активации используется бинарная  кроссэнтропия\n",
    "model_classification_1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"binary_crossentropy\")\n",
    "# verbose=None - не будет логов\n",
    "model_classification_1.fit(x_train_c, y_train_c, epochs=25, verbose=None)\n",
    "\n",
    "y_pred = np.around(model_classification_1.predict(x_test_c, verbose=None))\n",
    "\n",
    "print(classification_report(y_test_c, y_pred))\n",
    "print(confusion_matrix(y_test_c, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = 1 / y_train_c[y_train_c==0].shape[0]\n",
    "w1 = 1 / y_train_c[y_train_c==1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80      4099\n",
      "           1       0.79      0.80      0.80      4103\n",
      "\n",
      "    accuracy                           0.80      8202\n",
      "   macro avg       0.80      0.80      0.80      8202\n",
      "weighted avg       0.80      0.80      0.80      8202\n",
      "\n",
      "[[3248  851]\n",
      " [ 817 3286]]\n"
     ]
    }
   ],
   "source": [
    "model_classification_1 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\", input_dim = x_class.shape[1]),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.05),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_classification_1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"binary_crossentropy\")\n",
    "model_classification_1.fit(x_train_c, y_train_c, epochs=25, verbose=None, class_weight={0: w0, 1: w1})\n",
    "model_classification_1.predict(x_test_c, verbose=None)\n",
    "\n",
    "y_pred = np.around(model_classification_1.predict(x_test_c, verbose=None))\n",
    "print(classification_report(y_test_c, y_pred))\n",
    "print(confusion_matrix(y_test_c, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2af441d80d0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_classification_2 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\", input_dim = x_class.shape[1]),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(2, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_classification_2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), loss=\"sparse_categorical_crossentropy\")\n",
    "model_classification_2.fit(x_train_c, y_train_c, epochs=25, verbose=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.72      0.78      4099\n",
      "           1       0.76      0.87      0.81      4103\n",
      "\n",
      "    accuracy                           0.79      8202\n",
      "   macro avg       0.80      0.79      0.79      8202\n",
      "weighted avg       0.80      0.79      0.79      8202\n",
      "\n",
      "[[2948 1151]\n",
      " [ 540 3563]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = [np.argmax(pred) for pred in model_classification_2.predict(x_test_c, verbose=None)]\n",
    "print(classification_report(y_test_c, y_pred))\n",
    "print(confusion_matrix(y_test_c, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
